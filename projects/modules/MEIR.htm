<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="author" content="MARS">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name=keywords content="Mang Ye" , "Ye Mang" , "叶茫" , "WHU" , "Wuhan University" , "武汉大学" , "MARS" , "marswhu"
        , "MARS WHU">

    <title>Projects</title>

    <link href="../../static/bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="../../static/xin.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

</head>

<body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <span class="navbar-brand">
                    <font color="#ffffff">MARS</a></font>
                </span>
            </div>
            <div class="navbar-collapse collapse">
                <ul class="nav navbar-nav">
                    <li><a href="../../index.html">Home</a></li>
                    <li><a href="../../publications/index.htm"> Publications </a></li>
                    <li class="active"><a href="../index.htm"> Projects </a></li>
                    <li><a href="../../team/index.htm">Team</a></li>
                    <li><a href="../../teaching/index.htm">Teaching</a></li>
                    <li><a href="../../service/index.htm">Service</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container" style="margin-top: 50px;">
        <!-- <p>
            <br />
            <i class="bi-arrow-left-circle"></i><small><a href="#">Back</a></small>
        </p> -->
        <h2>Multimodal Emotional Understanding</h2>
        <div class="row-project">
            <div class="span12">
                <section>
                    <p>Multimodal Emotional Understanding involves analyzing and integrating information from
                        various modalities, such as text, audio, and visual cues, to accurately interpret human emotions
                        and intentions. This approach enhances human-computer interaction and enables more responsive
                        systems in applications like customer service, mental health monitoring, and social robotics,
                        ultimately improving the user experience by providing more nuanced and context-aware responses.
                    </p>
                    <p>We develop benchmarks for emotional understanding with Multimodal Large Language Models
                        (MLLMs). We also design frameworks that integrate different cues to enhance emotion and
                        intention recognition.</p>
                </section>
            </div>
        </div>

        <p>
            <i class="bi-box-arrow-in-left"></i><small><a style="text-decoration: none;" href="javascript:history.back(-1)">
                    <b>Back</b> </a></small>
        </p>

        <div class="page-header">
            <h3><i>Highlight</i></h3>
        </div>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/MEIR/EmoLLM.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>EmoLLM: Multimodal Emotional Understanding Meets Large Language
                                Models</strong></div>
                        <div class="pub-authors"> Qu Yang, <u>Mang Ye*</u>, Bo Du</div>
                        <div class="pub-venue"><i>preprint</i>
                        </div>

                        <br>
                        We introduce <b>EmoBench</b>, a comprehensive benchmark designed to enhance and evaluate the
                        emotional understanding capabilities of MLLMs across a diverse range of tasks, providing a
                        large-scale dataset of ~287k instructions. Besides, we propose <b>EmoLLM</b>, which incorporates
                        Multi-perspective Visual Projection to capture diverse emotional cues and EmoPrompt to guide the
                        reasoning process.


                        <div class="pub-description">
                            <a href="https://arxiv.org/pdf/2406.16442" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/yan9qu/EmoLLM" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/MEIR/ECCV24_IntCLIP.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Synergy of Sight and Semantics: Visual Intention Understanding
                                with CLIP</strong></div>
                        <div class="pub-authors"> Qu Yang, <u>Mang Ye*</u>, Dacheng Tao</div>
                        <div class="pub-venue"><i>European Conference on Computer Vision (ECCV), 2024.</i>
                        </div>

                        <br>
                        We introduce the IntCLIP framework, along with Hierarchical Class Integration, and the
                        Sight-assisted Aggregation to effectively facilitate this adaptation. Their synergy is
                        instrumental in guiding the extraction of intent cues and pinpointing key intention-related
                        areas.
                        <div class="pub-description">
                            <a href="../../publications/files/ECCV24_IntCLIP.pdf" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/yan9qu/IntCLIP" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/MEIR/CVPR24_CAGC.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">
                    <div class="pub-info">
                        <div class="pub-title"><strong>Contextual Augmented Global Contrast for Multimodal Intent
                                Recognition</strong></div>
                        <div class="pub-authors">Kaili Sun, Zhiwen Xie, <u>Mang Ye*</u>, Huyin Zhang </div>
                        <div class="pub-venue"><i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
                                Recognition (CVPR), 2024.</i>
                        </div>
                        <br>
                        We propose a context-augmented global contrast (CAGC) learning method to mine rich global
                        contextual cues from both intra-and cross-video to enhance intent understanding for multimodal
                        intention recognition.

                        <div class="pub-description">
                            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Contextual_Augmented_Global_Contrast_for_Multimodal_Intent_Recognition_CVPR_2024_paper.pdf"
                                target="_blank"><span class="label_download">Paper</span></a>
                            <a href="https://github.com/" target="_blank"><span class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <div class="row-fluid">
            <div class="span3">
                <div class="thumbnail">
                    <img class="scale-img" src="../images/MEIR/TAC23_HLEG.png" alt="">
                </div>
            </div>
            <div class="span9">
                <div class="pub-entry">

                    <div class="pub-info">
                        <div class="pub-title"><strong>Learnable Hierarchical Label Embedding and Grouping for Visual
                                Intention Understanding</strong></div>
                        <div class="pub-authors">QingHongYa Shi, <u>Mang Ye*</u>, Ziyi Zhang, Bo Du</div>
                        <div class="pub-venue"><i>IEEE Transactions on Affective Computing (TAC), 2023.</i>
                        </div>
                        <br>
                        We provide a novel learnable Hierarchical
                        Label Embedding and Grouping (HLEG) method for visual
                        intention understanding corresponding for limited data and label
                        ambiguity. It is featured in the following aspects: (1) Building a network with hierarchical
                        transformer structure according to the hierarchical label tree in visual intention
                        understanding. (2) Establishing the correlation between hierarchical and ambiguous labels
                        with learnable label embedding and grouping. (3) Introducing a “Hard-First” optimization
                        strategy between hierarchical classifications at multiple levels.

                        <div class="pub-description">
                            <a href="https://ieeexplore.ieee.org/abstract/document/10050120" target="_blank"><span
                                    class="label_download">Paper</span></a>
                            <a href="https://github.com/ShiQingHongYa/HLEG" target="_blank"><span
                                    class="label_download">Code</span></a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <hr>

        <p></p>
        </section>

        <div align="center">
            <small>Copyright &copy 2024 <a href="https://marswhu.github.io/">Multimedia Analysis & Reasoning (MARS)
                    Lab</a></small>
            <br>
            <small><a href="https://www.whu.edu.cn/">Wuhan University 武汉大学</a></small>
        </div>
    </div>


    <script src="../../static/jquery.js"></script>
    <script src="../../static/bootstrap/js/bootstrap.js"></script>
</body>

</html>